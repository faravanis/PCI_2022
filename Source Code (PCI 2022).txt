"""
================================================================================
 Title       : A Neuro-Symbolic Approach for Fault Diagnosis in Smart Power Grids
 Authors     : Theofanis Aravanis, Ioannis Kabouris
 Institution : University of the Peloponnese / University of Patras
 Conference  : 26th Pan-Hellenic Conference on Informatics (PCI 2022)
 DOI         : https://doi.org/10.1145/3575879.3575972
 Date        : 2022

 Description :
 This script implements the full data generation, processing, and classification
 pipeline as described in the paper:
 
     "A Neuro-Symbolic Approach for Fault Diagnosis in Smart Power Grids"
     Proceedings of the 26th Pan-Hellenic Conference on Informatics (PCI 2022)

 The implementation includes:
   - Simulation of voltage signal disturbances (sags, swells, transients, etc.)
   - Frequency-domain transformation using FFT
   - Classification using both MLP and 1D CNN models
   - Training with early stopping and evaluation via confusion matrices

 Usage :
 This code is provided for research and educational use. If you use or adapt
 this code in your work, please cite the original article using the provided DOI.

 License :
 Â© 2022 Theofanis Aravanis & Ioannis Kabouris. All rights reserved.
 This code is distributed for academic purposes only.

================================================================================
"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import random as rm
from scipy.fftpack import rfft, rfftfreq
from sklearn.utils import shuffle
from sklearn.metrics import confusion_matrix
import seaborn as sns
from tensorflow.keras.callbacks import EarlyStopping

# ==== Configuration ====
CLASS_NAMES = ['Normal', 'Interruption', 'Sag', 'Swell', 'Transient', 'Harmonics']
CLASS_INSTANCES = 500
AMPLITUDE = 325
FREQ = 50
DURATION = 0.5
SAMPLE_RATE = 5000
SAMPLES = int(DURATION * SAMPLE_RATE)
TIME = np.linspace(0, DURATION, SAMPLES)

# ==== EarlyStopping Callback ====
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

# ==== Signal Generator ====
def sine_wave(A, t):
    return A * np.sin(2 * np.pi * FREQ * t) + np.random.normal(0, 8)

def make_class_samples(class_id):
    samples = []
    for _ in range(CLASS_INSTANCES):
        if class_id == 0:  # Normal
            samples.append([sine_wave(AMPLITUDE, ti) for ti in TIME])

        elif class_id in [1, 2, 3]:  # Interruption, Sag, Swell
            start = rm.uniform(0.01, 0.5)
            length = rm.uniform(0.02, 0.12)

            def amp(ti):
                if start - length / 2 < ti < start + length / 2:
                    return [0, 0.4 * AMPLITUDE, 1.5 * AMPLITUDE][class_id - 1]
                return AMPLITUDE

            samples.append([sine_wave(amp(ti), ti) for ti in TIME])

        elif class_id == 4:  # Transient
            def amp(ti):
                return 2 * AMPLITUDE if abs(np.random.normal(0, 0.9)) > 2.6 else AMPLITUDE
            samples.append([sine_wave(amp(ti), ti) for ti in TIME])

        elif class_id == 5:  # Harmonics
            def harmonic(ti):
                return (AMPLITUDE * np.sin(2 * np.pi * FREQ * ti) +
                        (AMPLITUDE / 4) * np.sin(3 * 2 * np.pi * FREQ * ti) +
                        (AMPLITUDE / 6) * np.sin(5 * 2 * np.pi * FREQ * ti) +
                        (AMPLITUDE / 10) * np.sin(7 * 2 * np.pi * FREQ * ti))
            samples.append([harmonic(ti) + np.random.normal(0, 10) for ti in TIME])
    return samples

# ==== Dataset Creation ====
x_tr, y_tr = [], []
for cid in range(6):
    x_tr.extend(make_class_samples(cid))
    y_tr.extend([cid] * CLASS_INSTANCES)

# ==== Plot Sample Signal ====
plt.plot(TIME, x_tr[0])
plt.title("Sample Signal - Normal")
plt.xlabel("Time (s)")
plt.ylabel("Voltage (V)")
plt.grid(True)
plt.show()

# ==== Apply FFT ====
x_fft = [rfft(signal) for signal in x_tr]
rfftx = rfftfreq(SAMPLES, 1 / SAMPLE_RATE)

# Plot FFT
plt.plot(rfftx, np.abs(x_fft[0]) / (SAMPLES / 2))
plt.title("FFT of Sample Signal")
plt.xlabel("Frequency (Hz)")
plt.ylabel("Magnitude")
plt.grid(True)
plt.show()

# ==== Normalize & Shuffle ====
x_fft = np.array(x_fft)
x_fft = tf.keras.utils.normalize(x_fft, axis=1)
y = np.array(y_tr)
x_fft, y = shuffle(x_fft, y, random_state=42)

# ==== Train/Test Split ====
split_idx = int(0.7 * len(x_fft))
x_train, x_test = x_fft[:split_idx], x_fft[split_idx:]
y_train, y_test = y[:split_idx], y[split_idx:]

# ==== Define MLP Model ====
def build_mlp(input_shape):
    return tf.keras.Sequential([
        tf.keras.Input(shape=input_shape),
        tf.keras.layers.Dense(130, activation='relu'),
        tf.keras.layers.Dense(130, activation='relu'),
        tf.keras.layers.Dense(6, activation='softmax')
    ])

# ==== Train MLP ====
mlp = build_mlp(x_train.shape[1:])
mlp.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

mlp_history = mlp.fit(
    x_train, y_train,
    epochs=50,
    validation_data=(x_test, y_test),
    callbacks=[early_stop]
)

# ==== Plot MLP Metrics ====
plt.plot(mlp_history.history['accuracy'], label='Train')
plt.plot(mlp_history.history['val_accuracy'], label='Test')
plt.title("MLP Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.show()

plt.plot(mlp_history.history['loss'], label='Train')
plt.plot(mlp_history.history['val_loss'], label='Test')
plt.title("MLP Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()

# ==== Evaluate MLP ====
mlp_preds = np.argmax(mlp.predict(x_test), axis=1)
mlp_conf = confusion_matrix(y_test, mlp_preds)
sns.heatmap(mlp_conf, annot=True, fmt='g', cmap='Greens',
            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)
plt.title("MLP Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

# ==== Define CNN Model ====
def build_cnn(input_shape):
    return tf.keras.Sequential([
        tf.keras.Input(shape=input_shape),
        tf.keras.layers.Conv1D(64, 3, activation='relu'),
        tf.keras.layers.MaxPooling1D(2),
        tf.keras.layers.Conv1D(64, 3, activation='relu'),
        tf.keras.layers.MaxPooling1D(2),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(6, activation='softmax')
    ])

# ==== Reshape FFT for CNN ====
x_train_cnn = x_train[..., np.newaxis]
x_test_cnn = x_test[..., np.newaxis]

# ==== Train CNN ====
cnn = build_cnn(x_train_cnn.shape[1:])
cnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

cnn_history = cnn.fit(
    x_train_cnn, y_train,
    epochs=50,
    validation_data=(x_test_cnn, y_test),
    callbacks=[early_stop]
)

# ==== Plot CNN Metrics ====
plt.plot(cnn_history.history['accuracy'], label='Train')
plt.plot(cnn_history.history['val_accuracy'], label='Test')
plt.title("CNN Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.show()

plt.plot(cnn_history.history['loss'], label='Train')
plt.plot(cnn_history.history['val_loss'], label='Test')
plt.title("CNN Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()

# ==== Evaluate CNN ====
cnn_preds = np.argmax(cnn.predict(x_test_cnn), axis=1)
cnn_conf = confusion_matrix(y_test, cnn_preds)
sns.heatmap(cnn_conf, annot=True, fmt='g', cmap='Blues',
            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)
plt.title("CNN Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()
