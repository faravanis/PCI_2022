import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import random as rm
from scipy.fftpack import rfft, rfftfreq
from sklearn.utils import shuffle
from sklearn.metrics import confusion_matrix
import seaborn as sns

# ---- Global Configuration ----
CLASS_NAMES = ['Normal', 'Interruption', 'Sag', 'Swell', 'Transient', 'Harmonics']
CLASS_INSTANCES = 500
AMPLITUDE = 325
FREQ = 50
DURATION = 0.5
SAMPLE_RATE = 5000
SAMPLES = int(DURATION * SAMPLE_RATE)
TIME = np.linspace(0, DURATION, SAMPLES)

# ---- EarlyStopping Callback ----
early_stop = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

# ---- Signal Generator ----
def sine_wave(A, t):
    return A * np.sin(2 * np.pi * FREQ * t) + np.random.normal(0, 8)

def make_class_samples(class_id):
    signals = []
    for _ in range(CLASS_INSTANCES):
        if class_id == 0:  # Normal
            signals.append([sine_wave(AMPLITUDE, ti) for ti in TIME])

        elif class_id in [1, 2, 3]:  # Interruption, Sag, Swell
            start = rm.uniform(0.01, 0.5)
            length = rm.uniform(0.02, 0.12)
            def amplitude(ti):
                if start - length / 2 < ti < start + length / 2:
                    return [0, 0.4 * AMPLITUDE, 1.5 * AMPLITUDE][class_id - 1]
                return AMPLITUDE
            signals.append([sine_wave(amplitude(ti), ti) for ti in TIME])

        elif class_id == 4:  # Transient
            def amp(ti):
                return 2 * AMPLITUDE if abs(np.random.normal(0, 0.9)) > 2.6 else AMPLITUDE
            signals.append([sine_wave(amp(ti), ti) for ti in TIME])

        elif class_id == 5:  # Harmonics
            def harmonic(ti):
                return (AMPLITUDE * np.sin(2 * np.pi * FREQ * ti) +
                        (AMPLITUDE / 4) * np.sin(3 * 2 * np.pi * FREQ * ti) +
                        (AMPLITUDE / 6) * np.sin(5 * 2 * np.pi * FREQ * ti) +
                        (AMPLITUDE / 10) * np.sin(7 * 2 * np.pi * FREQ * ti))
            signals.append([harmonic(ti) + np.random.normal(0, 10) for ti in TIME])
    return signals

# ---- Data Generation ----
x_tr, y_tr = [], []
for cid in range(6):
    x_tr.extend(make_class_samples(cid))
    y_tr.extend([cid] * CLASS_INSTANCES)

# ---- Signal Preview ----
plt.plot(TIME, x_tr[0])
plt.title("Sample Signal - Normal")
plt.xlabel("Time (s)")
plt.ylabel("Voltage (V)")
plt.grid(True)
plt.show()

# ---- FFT Transformation ----
x_f = [rfft(signal) for signal in x_tr]
x_f = np.array(x_f)
x_f = tf.keras.utils.normalize(x_f, axis=1)
y = np.array(y_tr)

# ---- Shuffle & Split ----
x_f, y = shuffle(x_f, y, random_state=42)
split_idx = int(0.7 * len(x_f))
x_train, x_test = x_f[:split_idx], x_f[split_idx:]
y_train, y_test = y[:split_idx], y[split_idx:]

# ---- MLP Model ----
def build_mlp(input_shape):
    return tf.keras.Sequential([
        tf.keras.Input(shape=input_shape),
        tf.keras.layers.Dense(130, activation='relu'),
        tf.keras.layers.Dense(130, activation='relu'),
        tf.keras.layers.Dense(6, activation='softmax')
    ])

# ---- Train MLP ----
mlp = build_mlp(x_train.shape[1:])
mlp.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
mlp_hist = mlp.fit(x_train, y_train, epochs=50, validation_data=(x_test, y_test), callbacks=[early_stop])

# ---- Plot MLP Results ----
plt.plot(mlp_hist.history['accuracy'], label='Train')
plt.plot(mlp_hist.history['val_accuracy'], label='Test')
plt.title("MLP Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.show()

plt.plot(mlp_hist.history['loss'], label='Train')
plt.plot(mlp_hist.history['val_loss'], label='Test')
plt.title("MLP Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()

mlp_preds = np.argmax(mlp.predict(x_test), axis=1)
mlp_conf = confusion_matrix(y_test, mlp_preds)
sns.heatmap(mlp_conf, annot=True, fmt='g', cmap='Greens',
            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)
plt.title("MLP Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

# ---- CNN Model ----
def build_cnn(input_shape):
    return tf.keras.Sequential([
        tf.keras.Input(shape=input_shape),
        tf.keras.layers.Conv1D(64, 3, activation='relu'),
        tf.keras.layers.MaxPooling1D(2),
        tf.keras.layers.Conv1D(64, 3, activation='relu'),
        tf.keras.layers.MaxPooling1D(2),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(6, activation='softmax')
    ])

# ---- Reshape for CNN ----
x_train_cnn = x_train[..., np.newaxis]
x_test_cnn = x_test[..., np.newaxis]

# ---- Train CNN ----
cnn = build_cnn(x_train_cnn.shape[1:])
cnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
cnn_hist = cnn.fit(x_train_cnn, y_train, epochs=50, validation_data=(x_test_cnn, y_test), callbacks=[early_stop])

# ---- Plot CNN Results ----
plt.plot(cnn_hist.history['accuracy'], label='Train')
plt.plot(cnn_hist.history['val_accuracy'], label='Test')
plt.title("CNN Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.show()

plt.plot(cnn_hist.history['loss'], label='Train')
plt.plot(cnn_hist.history['val_loss'], label='Test')
plt.title("CNN Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()

cnn_preds = np.argmax(cnn.predict(x_test_cnn), axis=1)
cnn_conf = confusion_matrix(y_test, cnn_preds)
sns.heatmap(cnn_conf, annot=True, fmt='g', cmap='Blues',
            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)
plt.title("CNN Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()
